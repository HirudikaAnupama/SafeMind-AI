{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T22:46:11.258966Z",
     "start_time": "2024-12-11T22:46:09.794193Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:50:19.674367Z",
     "start_time": "2024-12-11T22:50:19.599740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read json file\n",
    "data = pd.read_json(\"data/combined_dataset.json\", lines=True)"
   ],
   "id": "50eee525d902619",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:50:48.399146Z",
     "start_time": "2024-12-11T22:50:48.383943Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "8bcef4d8a6f05f5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                Context  \\\n",
       "0     I'm going through some things with my feelings...   \n",
       "1     I'm going through some things with my feelings...   \n",
       "2     I'm going through some things with my feelings...   \n",
       "3     I'm going through some things with my feelings...   \n",
       "4     I'm going through some things with my feelings...   \n",
       "...                                                 ...   \n",
       "3507  My grandson's step-mother sends him to school ...   \n",
       "3508  My boyfriend is in recovery from drug addictio...   \n",
       "3509  The birth mother attempted suicide several tim...   \n",
       "3510  I think adult life is making him depressed and...   \n",
       "3511  I just took a job that requires me to travel f...   \n",
       "\n",
       "                                               Response  \n",
       "0     If everyone thinks you're worthless, then mayb...  \n",
       "1     Hello, and thank you for your question and see...  \n",
       "2     First thing I'd suggest is getting the sleep y...  \n",
       "3     Therapy is essential for those that are feelin...  \n",
       "4     I first want to let you know that you are not ...  \n",
       "...                                                 ...  \n",
       "3507  Absolutely not! It is never in a child's best ...  \n",
       "3508  I'm sorry you have tension between you and you...  \n",
       "3509  The true answer is, \"no one can really say wit...  \n",
       "3510  How do you help yourself to believe you requir...  \n",
       "3511                           hmm this is a tough one!  \n",
       "\n",
       "[3512 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>My grandson's step-mother sends him to school ...</td>\n",
       "      <td>Absolutely not! It is never in a child's best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>My boyfriend is in recovery from drug addictio...</td>\n",
       "      <td>I'm sorry you have tension between you and you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>The birth mother attempted suicide several tim...</td>\n",
       "      <td>The true answer is, \"no one can really say wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>I think adult life is making him depressed and...</td>\n",
       "      <td>How do you help yourself to believe you requir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>I just took a job that requires me to travel f...</td>\n",
       "      <td>hmm this is a tough one!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:06:42.979177Z",
     "start_time": "2024-12-11T23:06:42.963324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display dataset info\n",
    "data.info()"
   ],
   "id": "e7b4a62c60808fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2752 entries, 0 to 3511\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Context   2752 non-null   object\n",
      " 1   Response  2752 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 64.5+ KB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data cleaning\n",
    "#### Remove duplicates"
   ],
   "id": "dce9feb760412e4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:58:12.945634Z",
     "start_time": "2024-12-11T22:58:12.907995Z"
    }
   },
   "cell_type": "code",
   "source": "data.duplicated().sum()",
   "id": "b559bd4ee3b772c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:58:29.840039Z",
     "start_time": "2024-12-11T22:58:29.811628Z"
    }
   },
   "cell_type": "code",
   "source": "data.drop_duplicates(inplace=True)",
   "id": "d79f5a756daaffae",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:58:36.041360Z",
     "start_time": "2024-12-11T22:58:36.018191Z"
    }
   },
   "cell_type": "code",
   "source": "data.duplicated().sum()",
   "id": "49bed409e83e4ab7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove missing values",
   "id": "d935ec2b7c5faf32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:59:26.380522Z",
     "start_time": "2024-12-11T22:59:26.369506Z"
    }
   },
   "cell_type": "code",
   "source": "data.isna().sum()",
   "id": "29f033a37455ae71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context     0\n",
       "Response    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean the Context Text",
   "id": "c1d03ebcf1e4157a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:04:06.834781Z",
     "start_time": "2024-12-11T23:04:06.829331Z"
    }
   },
   "cell_type": "code",
   "source": "import re",
   "id": "fd511dbe2b8fc3c9",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:07:56.757441Z",
     "start_time": "2024-12-11T23:07:56.679727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove Extra Spaces, Tabs, and Newlines\n",
    "data['Context'] = data['Context'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()"
   ],
   "id": "87d7ba55bad95cab",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:10:05.872239Z",
     "start_time": "2024-12-11T23:10:05.820365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standardize Capitalization\n",
    "data['Context'] = data['Context'].str.lower()\n",
    "data['Context'] = data['Context'].str.replace(r\"(^\\w|\\.\\s*\\w)\", lambda m: m.group().upper(), regex=True)\n"
   ],
   "id": "cfd56a28660b7ca1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:10:06.104147Z",
     "start_time": "2024-12-11T23:10:06.021437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove Sensitive Data\n",
    "data['Context'] = data['Context'].str.replace(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\", \"[EMAIL]\", regex=True)\n",
    "data['Context'] = data['Context'].str.replace(r\"\\b\\d{10}\\b\", \"[PHONE NUMBER]\", regex=True)\n"
   ],
   "id": "3a2c95e9631f0f4f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:10:28.160319Z",
     "start_time": "2024-12-11T23:10:28.085581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize Punctuation\n",
    "data['Context'] = data['Context'].str.replace(r\"[?!]+\", lambda m: m.group()[0], regex=True)\n",
    "data['Context'] = data['Context'].str.replace(r\"([.,!?])(\\w)\", r\"\\1 \\2\", regex=True)\n",
    "data['Context'] = data['Context'].str.replace(r\"\\s([.,!?])\", r\"\\1\", regex=True)\n"
   ],
   "id": "dced7b5337c5a683",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean the Response Text",
   "id": "28f18ab1ca5c0b31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:11:41.974698Z",
     "start_time": "2024-12-11T23:11:41.711030Z"
    }
   },
   "cell_type": "code",
   "source": "data['Response'] = data['Response'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
   "id": "a1b9837ecb67e04a",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:11:54.422554Z",
     "start_time": "2024-12-11T23:11:54.272406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['Response'] = data['Response'].str.lower()\n",
    "data['Response'] = data['Response'].str.replace(r\"(^\\w|\\.\\s*\\w)\", lambda m: m.group().upper(), regex=True)\n"
   ],
   "id": "e30f7c9006940a28",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:12:06.494560Z",
     "start_time": "2024-12-11T23:12:06.256974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['Response'] = data['Response'].str.replace(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\", \"[EMAIL]\", regex=True)\n",
    "data['Response'] = data['Response'].str.replace(r\"\\b\\d{10}\\b\", \"[PHONE NUMBER]\", regex=True)\n"
   ],
   "id": "f22109431ecdc937",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:12:24.040084Z",
     "start_time": "2024-12-11T23:12:23.885321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['Response'] = data['Response'].str.replace(r\"[?!]+\", lambda m: m.group()[0], regex=True)\n",
    "data['Response'] = data['Response'].str.replace(r\"([.,!?])(\\w)\", r\"\\1 \\2\", regex=True)\n",
    "data['Response'] = data['Response'].str.replace(r\"\\s([.,!?])\", r\"\\1\", regex=True)\n"
   ],
   "id": "1d1d7b80ca3d44a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:12:53.233468Z",
     "start_time": "2024-12-11T23:12:53.153492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save to a new JSON file\n",
    "data.to_json(\"data/cleaned_dataset.json\", orient=\"records\", lines=True)\n",
    "\n",
    "print(\"Dataset cleaned and saved as 'data/cleaned_dataset.json'.\")\n"
   ],
   "id": "775c868fce7b50b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cleaned and saved as 'data/cleaned_dataset.json'.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:13:02.213420Z",
     "start_time": "2024-12-11T23:13:02.151378Z"
    }
   },
   "cell_type": "code",
   "source": "clean_data = pd.read_json(\"data/cleaned_dataset.json\", lines=True)",
   "id": "55ad624f8ebbf07e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:13:05.020568Z",
     "start_time": "2024-12-11T23:13:05.005230Z"
    }
   },
   "cell_type": "code",
   "source": "clean_data",
   "id": "b7f96c017a1092a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                Context  \\\n",
       "0     I'm going through some things with my feelings...   \n",
       "1     I'm going through some things with my feelings...   \n",
       "2     I'm going through some things with my feelings...   \n",
       "3     I'm going through some things with my feelings...   \n",
       "4     I'm going through some things with my feelings...   \n",
       "...                                                 ...   \n",
       "2747  After first meeting the client, what is the pr...   \n",
       "2748  My boyfriend is in recovery from drug addictio...   \n",
       "2749  The birth mother attempted suicide several tim...   \n",
       "2750  I think adult life is making him depressed and...   \n",
       "2751  I just took a job that requires me to travel f...   \n",
       "\n",
       "                                               Response  \n",
       "0     If everyone thinks you're worthless, then mayb...  \n",
       "1     Hello, and thank you for your question and see...  \n",
       "2     First thing i'd suggest is getting the sleep y...  \n",
       "3     Therapy is essential for those that are feelin...  \n",
       "4     I first want to let you know that you are not ...  \n",
       "...                                                 ...  \n",
       "2747  Hi. This is an excellent question! i think tha...  \n",
       "2748  I'm sorry you have tension between you and you...  \n",
       "2749  The true answer is, \"no one can really say wit...  \n",
       "2750  How do you help yourself to believe you requir...  \n",
       "2751                           Hmm this is a tough one!  \n",
       "\n",
       "[2752 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing i'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>After first meeting the client, what is the pr...</td>\n",
       "      <td>Hi. This is an excellent question! i think tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>My boyfriend is in recovery from drug addictio...</td>\n",
       "      <td>I'm sorry you have tension between you and you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>The birth mother attempted suicide several tim...</td>\n",
       "      <td>The true answer is, \"no one can really say wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>I think adult life is making him depressed and...</td>\n",
       "      <td>How do you help yourself to believe you requir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>I just took a job that requires me to travel f...</td>\n",
       "      <td>Hmm this is a tough one!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2752 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization",
   "id": "e40e0759a1a93d2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:16:37.230558Z",
     "start_time": "2024-12-11T23:16:26.109819Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer",
   "id": "58ebeddf9771fd84",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:18:33.039318Z",
     "start_time": "2024-12-11T23:18:32.428122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the tokenizer\n",
    "model_name = \"gpt2\"  # Replace with your desired model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Assign the eos_token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "3a7f512f3754b23a",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:18:40.705939Z",
     "start_time": "2024-12-11T23:18:38.096821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the input and output columns\n",
    "input_ids = tokenizer(list(clean_data[\"Context\"]), padding=True, truncation=True, max_length=512, return_tensors=\"pt\").input_ids\n",
    "output_ids = tokenizer(list(clean_data[\"Response\"]), padding=True, truncation=True, max_length=512, return_tensors=\"pt\").input_ids"
   ],
   "id": "fa2bb35f8aa722e0",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:19:11.702533Z",
     "start_time": "2024-12-11T23:19:11.694271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine into a single dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, output_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.output_ids = output_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"labels\": self.output_ids[idx],\n",
    "        }\n",
    "\n",
    "# Create the dataset object\n",
    "dataset = TextDataset(input_ids, output_ids)"
   ],
   "id": "26c76ead664c2d2c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fine-Tune the Model",
   "id": "2c1e3be0c3c87540"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:27:13.323255Z",
     "start_time": "2024-12-11T23:20:41.375765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ],
   "id": "6d5f0910f9ec6626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3455e07a1864fe4aa58d2605585b19a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e817d97102dc41e3af301a7107d84aa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Define training arguments\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./results\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./logs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Initialize the trainer\u001B[39;00m\n\u001B[0;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     18\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     19\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     20\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[0;32m     21\u001B[0m )\n",
      "File \u001B[1;32m<string>:134\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1780\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1778\u001B[0m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[1;32m-> 1780\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_tokens_across_devices:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:2306\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2303\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[0;32m   2304\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2305\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m-> 2306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, objtype)\u001B[0m\n\u001B[0;32m     58\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 60\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:2179\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[1;32m-> 2179\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   2180\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2181\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2182\u001B[0m         )\n\u001B[0;32m   2183\u001B[0m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[0;32m   2184\u001B[0m accelerator_state_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menabled\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_configured_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[1;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Save the Fine-Tuned Model\n",
   "id": "dbe14650d9a98b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:27:18.898422Z",
     "start_time": "2024-12-11T23:27:17.766944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Model and tokenizer saved!\")\n"
   ],
   "id": "4af4e826d7d3386e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved!\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####  Deploy as a Chatbot",
   "id": "98500b457b4080f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:51:21.508867Z",
     "start_time": "2024-12-11T23:49:27.730479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_saved_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the saved model and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load the model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading saved model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=200):\n",
    "    \"\"\"\n",
    "    Generate a response using the saved model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Decode the response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat interface to input questions and get responses.\n",
    "    \"\"\"\n",
    "    # Path to the saved model\n",
    "    model_path = \"./fine_tuned_model\"\n",
    "    \n",
    "    # Load the saved model and tokenizer\n",
    "    model, tokenizer = load_saved_model(model_path)\n",
    "    \n",
    "    if not model or not tokenizer:\n",
    "        print(\"Failed to load the saved model.\")\n",
    "        return\n",
    "    \n",
    "    print(\"🤖 Interactive Model Response\")\n",
    "    print(\"Type 'exit' to quit the program\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nEnter your question: \").strip()\n",
    "            \n",
    "            # Check for exit condition\n",
    "            if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"Goodbye! 👋\")\n",
    "                break\n",
    "            \n",
    "            # Generate and print response\n",
    "            if user_input:\n",
    "                response = generate_response(model, tokenizer, user_input)\n",
    "                print(\"\\nModel's Response:\", response)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nChat interrupted. Goodbye! 👋\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_chat()"
   ],
   "id": "19e29ea844d3a77d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Interactive Model Response\n",
      "Type 'exit' to quit the program\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Response: i am sad to say, but I am sorry for being so sad about this. I was never happy with myself. I had to make a choice.\n",
      "\n",
      "I can't help but be sad that my wife was not able to go through this. She was diagnosed with HIV, so I wanted to help her.\n",
      "\n",
      "The day before my appointment, my family and I had a baby girl. She was just 3 months old. I'm now 21 years old. I took her to the emergency room and my doctor told me she was in a good condition.\n",
      "\n",
      "I had to walk her through the front door to the hospital because it was so close to where I'm now. I told her the doctor I was going to take her to the emergency room and she said, \"Oh my God, this is so difficult. I'm so sorry. I'm so sorry. I can't wait to get back to my life.\"\n",
      "\n",
      "The day after, my family and I got\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Response: i am sad to say that I am not the one to be blamed. I am not a one who will be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed.\n",
      "\n",
      "I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am not the one to be blamed. I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Response: i am deppressed by the other side. I am not the first to notice the lack of a good reason for this.\n",
      "\n",
      "In the first place, I would have been surprised if the only reason why this group was not on the top of the list was because they weren't strong enough to handle the other side's attacks.\n",
      "\n",
      "But, the second thing is that the rest of the group is the same as well. The one who was stronger than the others had to be the strongest, and that was the reason they were ranked No. 2.\n",
      "\n",
      "If we compare them to the other side's members, we'll see that they are also the strongest.\n",
      "\n",
      "But, their strength would also be different.\n",
      "\n",
      "Even if the other side was stronger than them, they would still be ranked No. 3.\n",
      "\n",
      "\"Well, it's not like that. But, if we compare them to the other side's members, we'll see that they are also\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Response: i am depressed, and I'm sorry,\" she said.\n",
      "\n",
      "\"I'm not sure I want to leave. I don't want to leave my house,\" she said.\n",
      "\n",
      "The couple then left, leaving the couple in the living room, where they were still in the car with the couple's daughter.\n",
      "\n",
      "The couple was driving home to a family member's home in south Winnipeg when they heard a loud bang, the woman said.\n",
      "\n",
      "She said she got out of her car and saw a man in a black shirt running. She said she heard a woman screaming, \"What are you doing? I'm a child,\" she said.\n",
      "\n",
      "She said she thought she saw the man in the car and then saw a man in the street. She thought she saw him running, and then she saw a man outside the house.\n",
      "\n",
      "She said she saw the man running toward the house. She saw the man's car, and then saw a man in the house\n",
      "Goodbye! 👋\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:47:19.899259Z",
     "start_time": "2024-12-11T23:47:19.894490Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "83527064172822ec",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:47:19.904686Z",
     "start_time": "2024-12-11T23:47:19.901270Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fc6e73382220daa7",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c1692475755cc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
